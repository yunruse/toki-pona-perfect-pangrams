{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use both _pu_ ('official') and _ku suli_ ('unofficial' but highly recognised) words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "URL = \"https://lipu-linku.github.io/jasima/data.json\"\n",
    "data = json.loads(requests.get(URL).text)['data']\n",
    "words = [word for word, info in data.items()\n",
    "    if info['book'] in ('pu', 'ku suli')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove all the words that don't fit into our 17-char pangram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSONANTS = 'ptksmnljw'\n",
    "VOWELS = 'aeiou'\n",
    "wp = words_pangram = [w for w in words if all((\n",
    "    all(not w.startswith(v) for v in VOWELS),      # can't start with a vowel\n",
    "    all(not 'n'+v in w      for v in VOWELS),      # `n` should only end a syllable\n",
    "    all(w.count(c) <= 1     for c in CONSONANTS),  # no repeated consonants\n",
    "))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a set-based algorithm by looking at the consonants used. Let's keep track of those, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp2 = [(w, set(w) - set('aiueo')) for w in wp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! Now to evaluate. The generator below effectively jumps down a tree of all Toki Pona sentences from the words above, albeit only to a depth where the sentence is 17 letters, and skipping any words which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "LETTERS = set(VOWELS + CONSONANTS)\n",
    "PANGRAM_LENGTH = 17\n",
    "ALPHABETICAL = False\n",
    "\n",
    "def pangrams(\n",
    "    words: List[str] = [],\n",
    "    seen_consonants: set = set()\n",
    "):\n",
    "    done = False\n",
    "    all_letters = ''.join(words)\n",
    "    l = len(all_letters)\n",
    "    if l > PANGRAM_LENGTH:\n",
    "        return\n",
    "    if l == PANGRAM_LENGTH:\n",
    "        if set(all_letters) == LETTERS:\n",
    "            yield words\n",
    "    else:\n",
    "        i = 0\n",
    "        if ALPHABETICAL and words:\n",
    "            i = wp.index(words[-1])\n",
    "        for w, consonants in wp2[i:]:\n",
    "            if not seen_consonants & consonants:\n",
    "                yield from pangrams(words + [w], seen_consonants | consonants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ALPHABETICAL` constant trims down processing time by only looking at sentences which are in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pangrams():\n",
    "    print(' '.join(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the results of `ALPHABETICAL = True`, see the attached `.txt` file. If you want _every_ pangram – or you want _ku lili_ words or make 18-letter pangrams – hopefully this code isn't too much to modify!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
